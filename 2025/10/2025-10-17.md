# 2025-10-17 TIL

# âœ… ì˜¤ëŠ˜ í•œ ì¼
- AI íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ ê°œë…ê³¼ íŠ¹ì§• í•™ìŠµ
- CLIP ëª¨ë¸ì˜ êµ¬ì¡°ì™€ í•™ìŠµ ë°©ë²• ì´í•´
- Vision-Language Models (VLM)ì˜ ë‹¤ì–‘í•œ ì¢…ë¥˜ í•™ìŠµ
- Small VLM (sVLM)ê³¼ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ í•™ìŠµ
- ì´ë¯¸ì§€ íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ê³¼ Fine-tuning ê¸°ë²• ì´í•´
- í•©ì„± ë°ì´í„° í™œìš© ë°©ë²• ë° ìµœì‹  AI ë™í–¥ í•™ìŠµ

# ğŸ“š ë°°ìš´ ì 
- **íŒŒìš´ë°ì´ì…˜ ëª¨ë¸**: ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ í­ë„“ê²Œ í•™ìŠµí•œ í›„ ë‹¤ì–‘í•œ ë¬¸ì œì— ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆëŠ” ë²”ìš© ëŒ€í˜• AI ëª¨ë¸
- **CLIPì˜ í•µì‹¬**: Contrastive Learningìœ¼ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ë¥¼ í•™ìŠµí•˜ì—¬ Zero-shot ì „ì´ ê°€ëŠ¥
- **ë©€í‹°ëª¨ë‹¬ ì •í•©**: ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ê³µí†µëœ ì„ë² ë”© ë²¡í„° ê³µê°„ êµ¬ì„±ìœ¼ë¡œ ìœ ì‚¬ë„ ë¹„êµ ê°€ëŠ¥
- **LLaVA**: Vision Encoder + Projection Layer + LLM êµ¬ì¡°ë¡œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì´í•´
- **í† í°í™” íš¨ìœ¨ì„±**: ì˜ì–´ ì¤‘ì‹¬ í† í¬ë‚˜ì´ì €ëŠ” ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ì—ì„œ í† í° ë‚­ë¹„ ë°œìƒ (êµ¬ì¡°ì  ë¶ˆì´ìµ)
- **PEFTì˜ ì¥ì **: ì „ì²´ ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì§€ ì•Šê³  ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµí•˜ì—¬ íš¨ìœ¨ì ì¸ ì ì‘ ê°€ëŠ¥
- **Knowledge Distillation**: Teacher ëª¨ë¸ì˜ ì§€ì‹ì„ Student ëª¨ë¸ì— ì••ì¶•í•˜ì—¬ ê²½ëŸ‰í™”

# ğŸ“Œ í•µì‹¬ ê°œë…
- **íŒŒìš´ë°ì´ì…˜ ëª¨ë¸ì˜ 3ëŒ€ íŠ¹ì§•**: ëŒ€ê·œëª¨(Transformer + ëŒ€ê·œëª¨ ë°ì´í„°), ì ì‘ì„±(ë†’ì€ íŒŒì¸íŠœë‹ ì„±ëŠ¥), ë²”ìš©ì„±(ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ì§€ì›)
- **CLIP í•™ìŠµ ë°©ì‹**: Contrastive Learning - ì¼ì¹˜í•˜ëŠ” ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì€ ê°€ê¹ê²Œ, ë¶ˆì¼ì¹˜í•˜ëŠ” ìŒì€ ë©€ê²Œ ë°°ì¹˜
- **SigLIP**: Softmax ëŒ€ì‹  Sigmoid ê¸°ë°˜ ì†ì‹¤í•¨ìˆ˜ë¡œ ìŒì„± ë°ì´í„°ì— ì œí•œëœ ì˜í–¥ë§Œ ë°›ë„ë¡ ê°œì„ 
- **ViT (Vision Transformer)**: ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ Transformer ì ìš©, CLIPì˜ ì´ë¯¸ì§€ ì¸ì½”ë”ë¡œ ì‚¬ìš©
- **LLaVAì˜ 2ë‹¨ê³„ í•™ìŠµ**: (1) Projection Layer í•™ìŠµìœ¼ë¡œ í‘œí˜„ ê³µìœ , (2) Fine-tuningìœ¼ë¡œ íŠ¹ì • ì‘ì—… ê°•í™”
- **Qwen2-VLì˜ M-RoPE**: 1D í…ìŠ¤íŠ¸, 2D ì‹œê°, 3D ë¹„ë””ì˜¤ ìœ„ì¹˜ ì •ë³´ë¥¼ í†µí•© ì²˜ë¦¬
- **Set of Mark (SoM)**: ë‹¤ë¥¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ VLMì˜ ì‹œê° ëŠ¥ë ¥ ë³´ì™„
- **Learning Rate**: ì†ì‹¤í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ëª¨ë¸ì— ë°˜ì˜í•˜ëŠ” ë¹„ìœ¨, íŒŒì¸íŠœë‹ì—ì„œëŠ” ì‘ì€ ê°’ ì‚¬ìš©
- **í”„ë¡¬í”„íŠ¸ íŠœë‹**: í•™ìŠµ ê°€ëŠ¥í•œ ê°€ìƒ í† í°ì„ ì…ë ¥ì— ì¶”ê°€í•˜ì—¬ ëª¨ë¸ ê³ ì • ìƒíƒœë¡œ í•™ìŠµ
- **Adapter ëª¨ë“ˆ**: ì‘ì€ ëª¨ë“ˆì„ ì¶”ê°€í•˜ì—¬ Activation ë³€ê²½, íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹ ê°€ëŠ¥

# ğŸ’¡ ì˜ˆì‹œ ì½”ë“œ

**CLIPì„ ì‚¬ìš©í•œ Zero-shot ì´ë¯¸ì§€ ë¶„ë¥˜**
```python
import torch
import clip
from PIL import Image

# CLIP ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# ì´ë¯¸ì§€ ë¡œë“œ
image = preprocess(Image.open("image.jpg")).unsqueeze(0).to(device)

# í…ìŠ¤íŠ¸ ì¹´í…Œê³ ë¦¬ ì¤€ë¹„
text_inputs = torch.cat([
    clip.tokenize(f"a photo of a {c}") 
    for c in ["cat", "dog", "car", "bird"]
]).to(device)

# ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚°
with torch.no_grad():
    image_features = model.encode_image(image)
    text_features = model.encode_text(text_inputs)
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)

# ê²°ê³¼ ì¶œë ¥
values, indices = similarity[0].topk(4)
for value, index in zip(values, indices):
    print(f"{['cat', 'dog', 'car', 'bird'][index]}: {100 * value.item():.2f}%")
```

**LLaVA ìŠ¤íƒ€ì¼ ëª¨ë¸ êµ¬ì¡° (ê°œë…)**
```python
import torch
import torch.nn as nn

class SimpleLLaVA(nn.Module):
    def __init__(self, vision_encoder, projection_layer, language_model):
        super(SimpleLLaVA, self).__init__()
        
        self.vision_encoder = vision_encoder  # CLIP ViT
        self.projection = projection_layer     # Linear layer
        self.language_model = language_model   # LLM (e.g., Vicuna)
        
    def forward(self, images, text_inputs):
        # ì´ë¯¸ì§€ ì„ë² ë”©
        vision_features = self.vision_encoder(images)
        
        # ë¹„ì „ íŠ¹ì§•ì„ í…ìŠ¤íŠ¸ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜
        projected_features = self.projection(vision_features)
        
        # í…ìŠ¤íŠ¸ì™€ ê²°í•©
        combined_features = torch.cat([projected_features, text_inputs], dim=1)
        
        # ì–¸ì–´ ëª¨ë¸ë¡œ ìƒì„±
        output = self.language_model(combined_features)
        
        return output
```

**Contrastive Learning Loss (CLIP Loss ê°œë…)**
```python
import torch
import torch.nn.functional as F

def contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):
    # ì •ê·œí™”
    image_embeddings = F.normalize(image_embeddings, dim=-1)
    text_embeddings = F.normalize(text_embeddings, dim=-1)
    
    # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    logits = torch.matmul(image_embeddings, text_embeddings.t()) / temperature
    
    # ëŒ€ê°ì„  ìš”ì†Œê°€ ì •ë‹µ (ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ)
    batch_size = image_embeddings.shape[0]
    labels = torch.arange(batch_size).to(image_embeddings.device)
    
    # Cross-entropy loss (ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸, í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ì–‘ë°©í–¥)
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.t(), labels)
    
    loss = (loss_i2t + loss_t2i) / 2
    
    return loss
```

**LoRA (Low-Rank Adaptation) - PEFT ì˜ˆì‹œ**
```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=4):
        super(LoRALayer, self).__init__()
        
        # ì›ë³¸ ê°€ì¤‘ì¹˜ (ê³ ì •)
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False
        
        # Low-rank í–‰ë ¬ A, B (í•™ìŠµ ê°€ëŠ¥)
        self.lora_A = nn.Parameter(torch.randn(rank, in_features))
        self.lora_B = nn.Parameter(torch.randn(out_features, rank))
        
        self.scaling = 0.01
        
    def forward(self, x):
        # ì›ë³¸: W * x
        # LoRA: (W + scaling * B * A) * x
        original_output = F.linear(x, self.weight)
        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B)
        
        return original_output + self.scaling * lora_output
```

**Knowledge Distillation ì˜ˆì‹œ**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    def __init__(self, temperature=3.0, alpha=0.5):
        super(DistillationLoss, self).__init__()
        self.temperature = temperature
        self.alpha = alpha
        
    def forward(self, student_logits, teacher_logits, labels):
        # Hard target loss (ì‹¤ì œ ì •ë‹µ)
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # Soft target loss (Teacherì˜ soft label)
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)
        soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
        soft_loss *= (self.temperature ** 2)
        
        # ê²°í•©
        total_loss = self.alpha * soft_loss + (1 - self.alpha) * hard_loss
        
        return total_loss

# ì‚¬ìš© ì˜ˆì‹œ
teacher_model = TeacherModel()  # í° ëª¨ë¸
student_model = StudentModel()  # ì‘ì€ ëª¨ë¸

teacher_model.eval()  # TeacherëŠ” í‰ê°€ ëª¨ë“œ

distill_loss = DistillationLoss(temperature=3.0, alpha=0.7)

with torch.no_grad():
    teacher_logits = teacher_model(images)

student_logits = student_model(images)
loss = distill_loss(student_logits, teacher_logits, labels)
```

**Prompt Tuning ê°œë…**
```python
class PromptTuning(nn.Module):
    def __init__(self, base_model, n_prompts=10, embedding_dim=768):
        super(PromptTuning, self).__init__()
        
        self.base_model = base_model
        
        # í•™ìŠµ ê°€ëŠ¥í•œ ê°€ìƒ í† í° (Soft Prompt)
        self.soft_prompts = nn.Parameter(
            torch.randn(n_prompts, embedding_dim)
        )
        
        # Base ëª¨ë¸ì€ ê³ ì •
        for param in self.base_model.parameters():
            param.requires_grad = False
        
    def forward(self, input_embeddings):
        batch_size = input_embeddings.shape[0]
        
        # Soft promptë¥¼ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë³µì œ
        prompts = self.soft_prompts.unsqueeze(0).expand(
            batch_size, -1, -1
        )
        
        # ì…ë ¥ ì•ì— soft prompt ì¶”ê°€
        combined_input = torch.cat([prompts, input_embeddings], dim=1)
        
        # Base ëª¨ë¸ ìˆœì „íŒŒ
        output = self.base_model(combined_input)
        
        return output
```

**SAM (Segment Anything Model) ì‚¬ìš© ì˜ˆì‹œ**
```python
from segment_anything import sam_model_registry, SamPredictor
import cv2

# SAM ëª¨ë¸ ë¡œë“œ
sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")
predictor = SamPredictor(sam)

# ì´ë¯¸ì§€ ë¡œë“œ
image = cv2.imread("image.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# ì´ë¯¸ì§€ ì„¤ì •
predictor.set_image(image)

# í¬ì¸íŠ¸ í´ë¦­ìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜
input_point = np.array([[500, 375]])  # (x, y)
input_label = np.array([1])  # 1: ì „ê²½, 0: ë°°ê²½

masks, scores, logits = predictor.predict(
    point_coords=input_point,
    point_labels=input_label,
    multimask_output=True
)

# ê°€ì¥ ì¢‹ì€ ë§ˆìŠ¤í¬ ì„ íƒ
best_mask = masks[np.argmax(scores)]
```

# ğŸ› ï¸ ì´ìŠˆ & í•´ê²°
| ğŸ ë¬¸ì œ ìƒí™© | ğŸ” ì›ì¸ | ğŸ’¡ í•´ê²° ë°©ë²• |
|--------------|--------|--------------|
| ë¹„ì˜ì–´ê¶Œ ì–¸ì–´ì—ì„œ í† í° ë‚­ë¹„ ë°œìƒ | ì˜ì–´ ì¤‘ì‹¬ í† í¬ë‚˜ì´ì €ë¡œ ì¸í•œ êµ¬ì¡°ì  ë¶ˆì´ìµ | ì–¸ì–´ë³„ íŠ¹í™” í† í¬ë‚˜ì´ì € ê°œë°œ ë° ì–´íœ˜ í™•ì¥ (Qwen ì‹œë¦¬ì¦ˆ) |
| ëŒ€ê·œëª¨ ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡± | ì „ì²´ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸í•˜ë ¤ í•¨ | PEFT ê¸°ë²• ì‚¬ìš© (LoRA, Adapter, Prompt Tuning) |
| Learning Rateê°€ ë„ˆë¬´ ë†’ì•„ ë°œì‚° | íŒŒì¸íŠœë‹ ì‹œ ì´ë¯¸ ì¢‹ì€ ì‹œì‘ì ì—ì„œ ì‹œì‘ | ë³´ìˆ˜ì ì¸ ì‘ì€ learning rate ì‚¬ìš© (1e-5 ~ 1e-4) |
| VLMì˜ ì‹œê° ì´í•´ ëŠ¥ë ¥ ë¶€ì¡± | ë‹¨ì¼ ëª¨ë¸ì˜ í•œê³„ | Set of Mark (SoM)ë¡œ ë‹¤ë¥¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ê²°í•© |
| í•™ìŠµ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ | ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì˜ ì–´ë ¤ì›€ | í•©ì„± ë°ì´í„° ìƒì„± (GPT í™œìš©, ì‹œë®¬ë ˆì´ì…˜, Knowledge Distillation) |
| ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì„œ ì˜¨ë””ë°”ì´ìŠ¤ ë°°í¬ ë¶ˆê°€ | ëŒ€ê·œëª¨ íŒŒë¼ë¯¸í„°ë¡œ ì¸í•œ ë†’ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ | sVLM ê°œë°œ (ì–‘ìí™”, ê²½ëŸ‰í™”, Moondream 0.5B, Gemini Nano) |