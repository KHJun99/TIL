# 2025-10-15 TIL

# âœ… ì˜¤ëŠ˜ í•œ ì¼
- ì›Œë“œ ì„ë² ë”©(Word2Vec)ê³¼ ì›-í•« ì¸ì½”ë”© í•™ìŠµ
- RNNê³¼ LSTMì˜ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ ì´í•´
- Seq2Seqì™€ Attention ë©”ì»¤ë‹ˆì¦˜ í•™ìŠµ
- Self-Attentionê³¼ Transformer ì•„í‚¤í…ì²˜ í•™ìŠµ
- ì‚¬ì „í•™ìŠµ ëª¨ë¸(BERT, GPT) ë° In-Context Learning ê°œë… ì´í•´

# ğŸ“š ë°°ìš´ ì 
- **ì›-í•« ì¸ì½”ë”©ì˜ í•œê³„**: ì°¨ì›ì˜ ì €ì£¼, ì˜ë¯¸ì  ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë„ ë¬´ê´€í•œ ë²¡í„°ë¡œ ì·¨ê¸‰ë¨
- **ì›Œë“œ ì„ë² ë”©**: ë‹¨ì–´ë¥¼ ì£¼ë³€ ë‹¨ì–´ë“¤ë¡œ í‘œí˜„í•˜ë©´ ì˜ë¯¸ì  ê´€ê³„ë¥¼ í¬ì°©í•  ìˆ˜ ìˆëŠ” ë°€ì§‘ ë²¡í„° í‘œí˜„ ê°€ëŠ¥
- **RNNì˜ íŠ¹ì§•**: ì´ì „ ì‹œì ì˜ ì •ë³´ë¥¼ hidden stateì— ì €ì¥í•˜ì—¬ ìˆœì°¨ì  ë°ì´í„° ì²˜ë¦¬ì— ì í•©
- **LSTM**: Cell stateë¥¼ ì¶”ê°€í•˜ì—¬ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ê°€ëŠ¥, 3ê°€ì§€ ê²Œì´íŠ¸(Forget, Input, Output)ë¡œ ì •ë³´ ì œì–´
- **Seq2Seq**: Encoder-Decoder êµ¬ì¡°ë¡œ ê°€ë³€ ê¸¸ì´ ì…ì¶œë ¥ ì²˜ë¦¬, Teacher Forcingìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ
- **Attentionì˜ í•µì‹¬**: Bottleneck ë¬¸ì œ í•´ê²°, ë””ì½”ë”ê°€ í•„ìš”í•œ ì…ë ¥ ë¶€ë¶„ì— ì§ì ‘ ì ‘ê·¼í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
- **Self-Attention**: í•œ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œ ì§ì ‘ ìƒí˜¸ì‘ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- **Transformerì˜ ìš°ìˆ˜ì„±**: RNN ì—†ì´ Attentionë§Œìœ¼ë¡œ êµ¬ì„±, ë³‘ë ¬í™”ì™€ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµì— íš¨ìœ¨ì 
- **ì‚¬ì „í•™ìŠµì˜ ì¤‘ìš”ì„±**: ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ì¼ë°˜ì  ì–¸ì–´ íŒ¨í„´ í•™ìŠµ í›„ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— fine-tuning

# ğŸ“Œ í•µì‹¬ ê°œë…
- **Word2Vecì˜ ë‘ ê°€ì§€ ë°©ì‹**: Skip-gram(ì¤‘ì‹¬â†’ì£¼ë³€), CBOW(ì£¼ë³€â†’ì¤‘ì‹¬)
- **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ**: ì—­ì „íŒŒ ì‹œ ê³¼ê±° ì‹œì ì˜ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì ¸ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ì–´ë ¤ì›€
- **LSTMì˜ ê²Œì´íŠ¸**: Forget(ë²„ë¦´ ì •ë³´), Input(ìƒˆë¡œ ì“¸ ì •ë³´), Output(ë‚´ë³´ë‚¼ ì •ë³´) ê²°ì •
- **Seq2Seqì˜ í•™ìŠµ**: End-to-End í•™ìŠµ, Teacher Forcingìœ¼ë¡œ ì•ˆì •í™”
- **Beam Search**: ë§¤ ë‹¨ê³„ kê°œì˜ ìœ ë§í•œ í›„ë³´ ìœ ì§€í•˜ë©° ìµœì  ì‹œí€€ìŠ¤ íƒìƒ‰
- **Attention Score ê³„ì‚°**: Queryì™€ Keyì˜ ìœ ì‚¬ë„ â†’ Softmax â†’ Value ê°€ì¤‘í•©
- **Positional Encoding**: Self-Attentionì˜ ìˆœì„œ ì •ë³´ ë¶€ì¬ ë¬¸ì œ í•´ê²°
- **Multi-Head Attention**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì •ë³´ íŒŒì•… (ë¬¸ë²•, ì˜ë¯¸, ì‹œì œ ë“±)
- **Scaled Dot Product**: Queryì™€ Key ì°¨ì›ì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ gradient ì•ˆì •í™”
- **Masked Self-Attention**: ë¯¸ë˜ ë‹¨ì–´ ì°¸ì¡° ë°©ì§€ (-âˆë¡œ ë§ˆìŠ¤í‚¹)
- **Cross-Attention**: Decoderì˜ Query + Encoderì˜ Key, Valueë¡œ ì…ì¶œë ¥ ì—°ê²°
- **Masked LM (BERT)**: ì…ë ¥ í† í°ì˜ 15%ë¥¼ ë§ˆìŠ¤í‚¹í•˜ì—¬ ì–‘ë°©í–¥ ë¬¸ë§¥ í•™ìŠµ
- **In-Context Learning (GPT-3)**: Fine-tuning ì—†ì´ ì˜ˆì‹œë§Œìœ¼ë¡œ ìƒˆë¡œìš´ íƒœìŠ¤í¬ ìˆ˜í–‰

# ğŸ’¡ ì˜ˆì‹œ ì½”ë“œ

**ì›-í•« ì¸ì½”ë”©**
```python
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

words = ["hotel", "conference", "walk"]
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(words)

# ì›-í•« ì¸ì½”ë”©
onehot_encoder = OneHotEncoder(sparse_output=False)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

print(onehot_encoded)
# [[1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]]
```

**Word2Vec (Skip-gram) ê°œë…**
```python
# Gensimì„ ì‚¬ìš©í•œ Word2Vec ì˜ˆì‹œ
from gensim.models import Word2Vec

sentences = [
    ['problems', 'turning', 'into', 'banking', 'crises', 'as'],
    ['banking', 'system', 'needs', 'reform'],
]

# Skip-gram ëª¨ë¸ í•™ìŠµ
model = Word2Vec(sentences, vector_size=100, window=2, 
                 min_count=1, sg=1, workers=4)

# ë‹¨ì–´ ë²¡í„° í™•ì¸
vector = model.wv['banking']
print(f"Banking vector shape: {vector.shape}")

# ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°
similar_words = model.wv.most_similar('banking', topn=3)
print(similar_words)
```

**ê°„ë‹¨í•œ RNN êµ¬ì¡°**
```python
import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        
        # ì…ë ¥ -> Hidden
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        # Hidden -> ì¶œë ¥
        self.h2o = nn.Linear(hidden_size, output_size)
        self.activation = nn.Tanh()
        
    def forward(self, x, hidden):
        # x: (batch_size, input_size)
        # hidden: (batch_size, hidden_size)
        
        combined = torch.cat([x, hidden], dim=1)
        hidden = self.activation(self.i2h(combined))
        output = self.h2o(hidden)
        
        return output, hidden
    
    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_size)
```

**LSTM êµ¬ì¡° (PyTorch ë‚´ì¥)**
```python
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)
        
        # LSTM ìˆœì „íŒŒ
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # ë§ˆì§€ë§‰ hidden state ì‚¬ìš©
        output = self.fc(hidden[-1])  # (batch_size, output_dim)
        
        return output
```

**Attention ë©”ì»¤ë‹ˆì¦˜**
```python
class Attention(nn.Module):
    def __init__(self, hidden_size):
        super(Attention, self).__init__()
        self.attention = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)
        
    def forward(self, decoder_hidden, encoder_outputs):
        # decoder_hidden: (batch_size, hidden_size)
        # encoder_outputs: (batch_size, seq_len, hidden_size)
        
        seq_len = encoder_outputs.size(1)
        
        # Decoder hiddenì„ ë°˜ë³µí•˜ì—¬ encoder outputsì™€ concat
        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)
        
        # Attention score ê³„ì‚°
        energy = torch.tanh(self.attention(
            torch.cat([decoder_hidden, encoder_outputs], dim=2)
        ))
        
        attention_scores = self.v(energy).squeeze(2)
        
        # Softmaxë¡œ attention weights ê³„ì‚°
        attention_weights = torch.softmax(attention_scores, dim=1)
        
        # Context vector ê³„ì‚° (ê°€ì¤‘í•©)
        context = torch.bmm(
            attention_weights.unsqueeze(1), 
            encoder_outputs
        ).squeeze(1)
        
        return context, attention_weights
```

**Self-Attention (Scaled Dot-Product)**
```python
class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k
        
    def forward(self, Q, K, V, mask=None):
        # Q, K, V: (batch_size, seq_len, d_k)
        
        # Attention scores ê³„ì‚°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)
        
        # Masking (ì˜µì…˜)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Attention weights
        attention_weights = torch.softmax(scores, dim=-1)
        
        # Context vector
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
```

**Multi-Head Attention**
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
        
    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # Linear projection and split into heads
        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Apply attention
        output, attention_weights = self.attention(Q, K, V, mask)
        
        # Concatenate heads
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # Final linear projection
        output = self.W_o(output)
        
        return output, attention_weights
```

**Positional Encoding**
```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        return x + self.pe[:, :x.size(1)]
```

# ğŸ› ï¸ ì´ìŠˆ & í•´ê²°
| ğŸ ë¬¸ì œ ìƒí™© | ğŸ” ì›ì¸ | ğŸ’¡ í•´ê²° ë°©ë²• |
|--------------|--------|--------------|
| RNNì—ì„œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ ì‹¤íŒ¨ | ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¡œ ë¨¼ ê³¼ê±° ì •ë³´ ì „ë‹¬ ì–´ë ¤ì›€ | LSTMì˜ Cell stateì™€ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš© |
| Seq2Seqì˜ Bottleneck ë¬¸ì œ | ì…ë ¥ ë¬¸ì¥ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì••ì¶•í•˜ì—¬ ì •ë³´ ì†ì‹¤ | Attention ë©”ì»¤ë‹ˆì¦˜ ë„ì…ìœ¼ë¡œ í•„ìš”í•œ ë¶€ë¶„ ì§ì ‘ ì°¸ì¡° |
| Self-Attentionì˜ ìˆœì„œ ì •ë³´ ë¶€ì¬ | ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë§Œ ê³„ì‚°í•˜ì—¬ ìœ„ì¹˜ ì •ë³´ ì†ì‹¤ | Positional Encoding ì¶”ê°€ (Sinusoidal ë˜ëŠ” Learned) |
| Self-Attentionì˜ ë¹„ì„ í˜•ì„± ë¶€ì¡± | ê°€ì¤‘í•© ì—°ì‚°ë§Œìœ¼ë¡œëŠ” ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ í•œê³„ | Feed-Forward Network ì¶”ê°€ (FC + ReLU) |
| ì–¸ì–´ ëª¨ë¸ì—ì„œ ë¯¸ë˜ ë‹¨ì–´ ì°¸ì¡° ë¬¸ì œ | Self-Attentionì´ ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì— ì°¸ì¡° | Masked Self-Attention ì‚¬ìš© (ë¯¸ë˜ ìœ„ì¹˜ë¥¼ -âˆë¡œ ë§ˆìŠ¤í‚¹) |
| Attention ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ gradient ë¶ˆì•ˆì • | Queryì™€ Key ì°¨ì›ì´ í´ìˆ˜ë¡ ë‚´ì  ê°’ ì¦ê°€ | Scaled Dot-Product (âˆšd_kë¡œ ë‚˜ëˆ„ê¸°) ì‚¬ìš© |
| ê¹Šì€ Transformer í•™ìŠµ ì–´ë ¤ì›€ | Gradient vanishing/exploding ë¬¸ì œ | Residual Connection + Layer Normalization ì ìš© |