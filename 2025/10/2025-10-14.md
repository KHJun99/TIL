# 2025-10-14 TIL

# ✅ 오늘 한 일
- 선형회귀와 로지스틱 회귀의 개념과 수식 학습
- 신경망(Shallow/Deep Network)의 구조와 표현력 이해
- 경사 하강법과 확률적 경사 하강법(SGD) 학습
- 역전파(Backpropagation) 알고리즘의 원리 이해
- 손실함수와 최적화 과정 학습

# 📚 배운 점
- **선형회귀**: 입력 변수와 출력 변수 사이의 관계를 직선 형태로 근사하여 예측하는 통계적 방법
- **다중선형회귀**: 여러 독립 변수를 동시에 고려하여 종속 변수와의 관계를 구하는 회귀 분석 기법
- **로지스틱 회귀**: 시그모이드 함수를 활용해 0~1 범위 내 확률값 예측을 보장하는 분류 방법
- **Shallow 네트워크**: Hidden Unit을 충분히 많이 두면 임의의 1차원 함수를 원하는 정확도로 근사 가능 (보편적 근사 정리)
- **Deep 네트워크**: 여러 층을 쌓아 복잡한 함수를 표현하며, 접기(folding) 개념으로 입력 공간을 변형
- **경사 하강법**: 손실 함수의 기울기 반대 방향으로 파라미터를 업데이트하여 최적값 탐색
- **SGD의 장점**: 무작위성으로 인해 Local Minima에서 빠져나올 수 있고, 계산 비용이 절감됨

# 📌 핵심 개념
- **최소제곱법(Least Squares)**: 실제 관측값과 예측값의 차이(잔차)를 제곱해 합한 값(RSS)을 최소화
- **정규방정식**: β̂ = (X^T X)^(-1) X^T y (다중선형회귀의 closed-form solution)
- **시그모이드 함수**: y = 1 / (1 + e^(-z)), 모든 실수 입력에 대해 0~1 사이 값 출력
- **오즈(Odds)**: 성공 확률 / 실패 확률
- **로짓 변환(Logit)**: log(odds) = β₀ + β₁x (로지스틱 회귀를 선형 모형으로 변환)
- **MLE(Maximum Likelihood Estimation)**: 우도(Likelihood)를 최대화하여 모수 추정
- **Hidden Units**: 입력층과 출력층 사이의 중간 계산 노드, 복잡한 패턴 학습 가능
- **보편적 근사 정리**: Hidden unit을 충분히 많이 갖는 얕은 신경망은 임의의 연속함수를 근사 가능
- **학습률(Learning Rate, α)**: 파라미터 업데이트 시 이동하는 크기를 결정하는 하이퍼파라미터
- **Convex vs Non-convex**: Convex는 전역 최소값이 유일하여 최적화가 쉽지만, Non-convex는 Local Minima가 존재하여 어려움
- **역전파(Backpropagation)**: 출력 오차를 기준으로 연쇄법칙을 사용하여 각 파라미터의 미분값을 계산

# 💡 예시 코드

**단순선형회귀 구현**
```python
import numpy as np

# 최소제곱법으로 계수 추정
def simple_linear_regression(X, y):
    x_mean = np.mean(X)
    y_mean = np.mean(y)
    
    # 기울기
    beta_1 = np.sum((X - x_mean) * (y - y_mean)) / np.sum((X - x_mean) ** 2)
    
    # 절편
    beta_0 = y_mean - beta_1 * x_mean
    
    return beta_0, beta_1

# 예측
def predict(X, beta_0, beta_1):
    return beta_0 + beta_1 * X

# MSE 계산
def calculate_mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)
```

**다중선형회귀 구현**
```python
# 정규방정식 사용
def multiple_linear_regression(X, y):
    # X에 절편을 위한 1열 추가
    X_bias = np.c_[np.ones((X.shape[0], 1)), X]
    
    # β̂ = (X^T X)^(-1) X^T y
    beta = np.linalg.inv(X_bias.T @ X_bias) @ X_bias.T @ y
    
    return beta

# 예측
def predict_multiple(X, beta):
    X_bias = np.c_[np.ones((X.shape[0], 1)), X]
    return X_bias @ beta
```

**시그모이드 함수와 로지스틱 회귀**
```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression_predict(X, beta):
    z = X @ beta
    return sigmoid(z)

# 로그 우도 함수
def log_likelihood(X, y, beta):
    p = logistic_regression_predict(X, beta)
    return np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))
```

**경사 하강법 구현**
```python
def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    n_samples, n_features = X.shape
    beta = np.zeros(n_features)
    
    for epoch in range(epochs):
        # 예측값 계산
        y_pred = X @ beta
        
        # 손실 계산 (MSE)
        loss = np.mean((y_pred - y) ** 2)
        
        # 기울기 계산
        gradient = (2 / n_samples) * X.T @ (y_pred - y)
        
        # 파라미터 업데이트
        beta = beta - learning_rate * gradient
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    return beta
```

**확률적 경사 하강법(SGD) 구현**
```python
def sgd(X, y, learning_rate=0.01, epochs=1000, batch_size=32):
    n_samples, n_features = X.shape
    beta = np.zeros(n_features)
    
    for epoch in range(epochs):
        # 데이터 셔플
        indices = np.random.permutation(n_samples)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        # 미니배치로 학습
        for i in range(0, n_samples, batch_size):
            X_batch = X_shuffled[i:i+batch_size]
            y_batch = y_shuffled[i:i+batch_size]
            
            # 예측 및 기울기 계산
            y_pred = X_batch @ beta
            gradient = (2 / batch_size) * X_batch.T @ (y_pred - y_batch)
            
            # 파라미터 업데이트
            beta = beta - learning_rate * gradient
    
    return beta
```

**Shallow 네트워크 순전파**
```python
def relu(x):
    return np.maximum(0, x)

def shallow_network(x, weights, biases):
    # Hidden layer
    h = relu(weights[0] @ x + biases[0])
    
    # Output layer
    y = weights[1] @ h + biases[1]
    
    return y
```

**역전파 예시 (간단한 2층 네트워크)**
```python
def backpropagation_example(X, y, weights, biases, learning_rate=0.01):
    # 순전파
    z1 = weights[0] @ X + biases[0]
    h1 = relu(z1)
    z2 = weights[1] @ h1 + biases[1]
    y_pred = z2
    
    # 손실 (MSE)
    loss = np.mean((y_pred - y) ** 2)
    
    # 역전파
    # 출력층 기울기
    d_loss_d_z2 = 2 * (y_pred - y)
    d_loss_d_w2 = d_loss_d_z2 @ h1.T
    d_loss_d_b2 = np.sum(d_loss_d_z2, axis=1, keepdims=True)
    
    # Hidden층 기울기
    d_loss_d_h1 = weights[1].T @ d_loss_d_z2
    d_loss_d_z1 = d_loss_d_h1 * (z1 > 0)  # ReLU 미분
    d_loss_d_w1 = d_loss_d_z1 @ X.T
    d_loss_d_b1 = np.sum(d_loss_d_z1, axis=1, keepdims=True)
    
    # 파라미터 업데이트
    weights[1] -= learning_rate * d_loss_d_w2
    biases[1] -= learning_rate * d_loss_d_b2
    weights[0] -= learning_rate * d_loss_d_w1
    biases[0] -= learning_rate * d_loss_d_b1
    
    return weights, biases, loss
```

# 🛠️ 이슈 & 해결
| 🐞 문제 상황 | 🔍 원인 | 💡 해결 방법 |
|--------------|--------|--------------|
| 선형회귀를 분류 문제에 적용 시 예측값이 0~1 범위를 벗어남 | 선형 함수는 출력 범위에 제한이 없음 | 로지스틱 회귀(시그모이드 함수)를 사용하여 0~1 범위로 제한 |
| 경사 하강법이 Local Minima에 빠져 최적값을 찾지 못함 | Non-convex 손실 함수에서 지역 최소값 존재 | 확률적 경사 하강법(SGD)을 사용하여 무작위성으로 탈출 가능 |
| 전체 데이터로 경사 하강 시 계산 비용이 너무 큼 | 매 스텝마다 모든 데이터의 기울기를 계산 | 미니배치 SGD로 일부 데이터만 사용하여 계산량 감소 |
| 학습률이 너무 크면 발산, 너무 작으면 학습이 느림 | 부적절한 학습률 설정 | 학습률 스케줄링 또는 적응적 학습률(Adam, RMSprop) 사용 |