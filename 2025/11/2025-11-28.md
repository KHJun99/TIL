# 2025-11-28 TIL

# ✅ 오늘 한 일
- 문서 유사도 개념 및 계산 방법 학습
- 코사인 유사도 개념 및 계산 방식 이해
- 카운트 기반 문서 유사도(BOW) 학습 및 실습
- 임베딩 기반 문서 유사도 개념 학습
- Word2Vec 원리 및 활용 실습
- Doc2Vec 원리 및 활용 실습
- OpenAI API를 활용한 임베딩 실습
- Upstage AI API를 활용한 임베딩 실습
- 임베딩 기반 도서 추천 시스템 구현

# 📚 배운 점
- 문서 유사도는 여러 문서가 얼마나 비슷한 내용을 담고 있는지를 수치로 표현한 것
- 코사인 유사도는 두 벡터가 이루는 각도의 코사인 값을 이용하며, 1에 가까울수록 유사도가 높음
- BOW(Bag of Words)는 단어의 등장 횟수를 기반으로 벡터를 만들지만, 의미적 유사도를 반영하지 못하는 한계가 있음
- Word2Vec은 주변 단어를 보고 중심 단어를 학습하여 의미가 비슷한 단어가 벡터 공간에서 가까이 위치하도록 함
- Word2Vec의 CBOW 방식은 주변 단어로 중심 단어를 예측하고, Skip-gram은 중심 단어로 주변 단어를 예측
- Doc2Vec은 문서 전체를 하나의 벡터로 표현하여 문서 간 유사도를 직접 계산할 수 있음
- 불용어 제거는 문서/도메인의 특성을 잘 반영해야 하며, 법률 문서에서는 '제, 조, 항' 등도 불용어가 될 수 있음
- 생성형 AI 기반 임베딩(OpenAI, Upstage)은 복잡한 전처리 없이 의미가 반영된 고품질 벡터를 얻을 수 있음
- Gensim 라이브러리는 Word2Vec, Doc2Vec 등 임베딩 알고리즘을 지원함
- Konlpy는 한국어 형태소 분석과 불용어 제거를 지원하는 NLP 라이브러리

# 📌 핵심 개념
- **문서 유사도**: 여러 문서가 얼마나 비슷한 내용을 담고 있는지를 수치로 표현
- **코사인 유사도**: 두 벡터가 이루는 각도의 코사인 값으로 유사도를 계산 (-1 ~ 1, 1에 가까울수록 유사)
- **벡터**: 방향과 크기를 나타내는 수학적 표현으로, 여러 수치를 일렬로 나열한 것
- **BOW (Bag of Words)**: 단어의 등장 횟수를 세어 벡터로 만드는 카운트 기반 방식
- **워드 임베딩 (Word Embedding)**: 텍스트를 의미를 반영한 숫자 벡터로 변환하는 기법
- **Word2Vec**: 분포 가설 기반으로 단어의 의미를 벡터로 학습하는 모델
- **CBOW**: 주변 단어들을 입력으로 받아 중심 단어를 예측하는 Word2Vec 학습 방식
- **Skip-gram**: 중심 단어를 입력으로 받아 주변 단어들을 예측하는 Word2Vec 학습 방식
- **Doc2Vec**: 문서 전체를 하나의 벡터로 표현하는 도큐먼트 임베딩 기법
- **불용어**: 분석에 도움이 되지 않는 단어들 (조사, 접속사 등)
- **형태소 분석**: 문장을 의미 있는 최소 단위로 분리하는 작업
- **토큰화**: 문장을 단어나 형태소 단위로 분리하는 작업

# 💡 예시 코드

```python
# 1. 코사인 유사도 계산
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(A, B):
    return dot(A, B) / (norm(A) * norm(B))

# 벡터 A(3, 3), 벡터 B(4, 1)
A = [3, 3]
B = [4, 1]
similarity = cosine_similarity(A, B)
print(f'코사인 유사도: {similarity}')  # 0.857...
```

```python
# 2. BOW (Bag of Words) 구현
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# 문서 정의
docs = [
    "먹고 싶은 사과",
    "먹고 싶은 바나나",
    "길고 노란 바나나 바나나",
    "저는 과일이 좋아요"
]

# CountVectorizer로 단어 카운트 행렬 생성
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(docs)

# numpy 배열로 변환
doc_term_matrix = X.toarray()

# 단어 목록 확인
vocab = vectorizer.get_feature_names_out()
print("단어 목록:", vocab)
print("\n문서-단어 행렬:\n", doc_term_matrix)

# 문서간 코사인 유사도 계산
for i in range(len(doc_term_matrix)):
    for j in range(i+1, len(doc_term_matrix)):
        print(f'문서{i+1}과 문서{j+1}의 유사도:')
        print(cosine_similarity(doc_term_matrix[i], doc_term_matrix[j]))
        print('-'*30)
```

```python
# 3. Word2Vec 활용
from gensim.models.word2vec import Word2Vec
from konlpy.tag import Okt
import re

# 데이터 로드
with open("대한민국헌법.txt", encoding='cp949') as f:
    content = f.read()

# 문장별로 분리
docs = re.split(r'(?<=[.?!])\s+', content.strip())

# 한글 외 문자 제거
docs = [re.sub(r'[^ㄱ-ㅎ가-힣 ]', '', doc) for doc in docs]

# 불용어 정의
stopwords = ['을', '의', '가', '이', '은', '들', '는', '좀', '잘', '걸', '과', '도', '를', '으로']

# 형태소 분석 및 토큰화
okt = Okt()
tokenized_data = []
for sentence in docs:
    tokenized_sentence = okt.morphs(sentence, stem=True)
    # 불용어 제거
    sentence = [word for word in tokenized_sentence if word not in stopwords]
    tokenized_data.append(sentence)

# Word2Vec 모델 학습
model = Word2Vec(
    sentences=tokenized_data,  # 토큰화된 문장들
    vector_size=100,           # 단어 벡터 차원 (100차원)
    window=5,                  # 주변 단어 윈도우 크기
    min_count=1,               # 최소 등장 횟수
    workers=4,                 # 병렬 처리 쓰레드 수
    sg=0                       # 0: CBOW, 1: Skip-Gram
)

# 가장 많은 빈도수를 보이는 단어 50개
print(model.wv.index_to_key[:50])

# 특정 단어의 벡터 확인
print("'법률' 단어의 100차원 벡터:")
print(model.wv.get_vector("법률"))

# 단어 간 코사인 유사도
similarity_value = cosine_similarity(
    model.wv.get_vector('법률'), 
    model.wv.get_vector('헌법')
)
print("'법률'과 '헌법'의 코사인 유사도:", similarity_value)

# 특정 단어와 가장 유사한 단어 출력
most_similar_words = model.wv.most_similar('헌법')
print("헌법과 유사한 단어 Top 10:", most_similar_words)
```

```python
# 4. Doc2Vec 활용
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from konlpy.tag import Okt

# 한국어 문장 모음
docs = [
    "자바스크립트는 웹 개발에 필수적인 프로그래밍 언어입니다.",
    "일본은 벚꽃이 피는 봄이 관광하기 좋습니다.",
    "파이썬 언어는 데이터분석과 기계학습에 활용됩니다.",
    "기계학습은 데이터를 활용하여 컴퓨터가 학습합니다.",
    "스페인은 날씨가 온화한 봄이나 가을에 방문하기 좋습니다."
]

okt = Okt()

# 각 문서에 태그 부여
documents = []
for i, doc in enumerate(docs):
    documents.append(TaggedDocument(tags=[i], words=okt.morphs(doc)))

print('문서의 수:', len(documents))

# Doc2Vec 모델 생성
model = Doc2Vec(
    vector_size=300,      # 벡터 차원
    min_count=1,          # 단어 최소 빈도수
    alpha=0.025,          # 학습률
    min_alpha=0.025,      # 학습률 초기값
    window=8              # 문맥 크기
)

# Vocabulary 빌드 및 학습
model.build_vocab(documents)
model.train(documents, total_examples=model.corpus_count, epochs=20)

# 첫번째 문장과 가장 유사한 문장 찾기
similar_docs = model.dv.most_similar(0)
print("첫번째 문장과 유사한 문장들:", similar_docs)
```

```python
# 5. OpenAI API 활용 임베딩
from openai import OpenAI

client = OpenAI(api_key="YOUR_OPENAI_API_KEY")

passage_list = [
    "먹고 싶은 사과",
    "먹고 싶은 바나나",
    "길고 노란 바나나 바나나",
    "봄이 와서 날씨가 좋습니다"
]

# 임베딩 요청
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=passage_list
).data

# 임베딩 벡터 추출
passage_embedding_list = [item.embedding for item in response]

# 벡터 차원 확인
print("첫 번째 문장의 벡터 차원:", len(passage_embedding_list[0]))  # 1536

# 코사인 유사도 계산
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

embedding_vectors = np.array(passage_embedding_list)
similarities = cosine_similarity(embedding_vectors, embedding_vectors)
print("코사인 유사도 행렬:\n", similarities)
```

```python
# 6. Upstage AI API 활용 임베딩
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_UPSTAGE_API_KEY",
    base_url="https://api.upstage.ai/v1/solar"
)

doc5_titles = [
    "자바스크립트언어",
    "일본관광시기",
    "파이썬언어",
    "기계학습기초",
    "스페인방문계절"
]

doc5_desc = [
    "자바스크립트는 웹 개발에 필수적인 프로그래밍 언어입니다.",
    "일본은 벚꽃이 피는 봄이 관광하기 좋습니다.",
    "파이썬 언어는 데이터분석과 기계학습에 활용됩니다.",
    "기계학습은 데이터를 활용하여 컴퓨터가 학습합니다.",
    "스페인은 날씨가 온화한 봄이나 가을에 방문하기 좋습니다."
]

# 임베딩 요청
response = client.embeddings.create(
    model="embedding-passage",
    input=doc5_desc
).data

# 임베딩 벡터 추출
embedding_vectors = [i.embedding for i in response]
print("벡터 개수:", len(embedding_vectors))
print("벡터 차원:", len(embedding_vectors[0]))  # 4096

# 코사인 유사도 계산
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity(embedding_vectors, embedding_vectors)
print("코사인 유사도 행렬:\n", similarities)

# 도서 추천 함수
def recommendations(title):
    if title in doc5_titles:
        idx = doc5_titles.index(title)
        similar_doc = similarities[idx]
        
        # 유사도 기준으로 정렬
        sim_scores = list(enumerate(similar_doc))
        sim_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 자기 자신 제외한 상위 3개
        similar_books_titles = []
        for index, score in sim_scores[1:4]:
            similar_books_titles.append(doc5_titles[index])
        
        return similar_books_titles
    else:
        print("도서 정보가 존재하지 않습니다.")
        return []

# 추천 결과
print(recommendations("자바스크립트언어"))
```

```python
# 7. pickle을 사용한 벡터 데이터 저장 및 로드
import pickle

# 저장
with open("embedding_vectors.pickle", "wb") as f:
    pickle.dump(embedding_vectors, f)

with open("similarities.pickle", "wb") as f:
    pickle.dump(similarities, f)

# 로드
with open("embedding_vectors.pickle", "rb") as f:
    embedding_vectors = pickle.load(f)

with open("similarities.pickle", "rb") as f:
    similarities = pickle.load(f)
```

# 🛠️ 이슈 & 해결
| 🐞 문제 상황 | 🔍 원인 | 💡 해결 방법 |
|--------------|--------|--------------|
| Gensim 설치 후 numpy 버전 오류 발생 | Gensim이 numpy 1.26.4를 사용하지만 Colab 세션은 2.0.2 사용 중 | 런타임 > '세션 다시 시작'을 실행하여 numpy 버전을 1.26.4로 갱신 |
| BOW 방식으로 '사과'와 '바나나' 문서의 유사도가 0으로 계산됨 | 카운트 기반 방식은 의미적 유사도를 반영하지 못함 | Word2Vec이나 생성형 AI 기반 임베딩을 사용하여 의미가 반영된 벡터로 변환 |
| Word2Vec 학습 후 가장 빈도수가 높은 단어에 '을', '제' 등이 포함됨 | 불용어 제거가 충분하지 않음 | 불용어 목록에 '을', '제', '조', '항' 등 도메인 특성을 반영한 단어 추가 |
| Doc2Vec으로 유사 문장을 찾았는데 결과가 만족스럽지 않음 | 문장이 너무 짧고 학습 데이터가 부족함 | 충분한 양의 긴 문장 데이터를 확보하고, 학습 파라미터(epochs, vector_size 등) 조정 |
| OpenAI API 호출 시 비용이 많이 발생할까 걱정됨 | text-embedding-3-large 모델은 비용이 높음 | text-embedding-3-small 모델 사용 (62,500 pages per dollar로 가장 저렴) |